anim = staticplot + transition_states(year_suicide$year, transition_length = 4, state_length = 1) +
view_follow(fixed_x = TRUE)  +
labs(title = 'Suicide Number : {closest_state}',
subtitle  =  "Top 10 Countries",
caption  = "Suicide Rates in 100k")
anim
animate(anim, 200, fps = 20,  width = 1200, height = 1000,
renderer = gifski_renderer("gganim.gif"))
})
year_suicide <- sqldf('select year,sum(suicidesper100k) as total from suicide4 group by year')
head(year_suicide)
ggplot(year_suicide, aes(x=year, y=total)) +
geom_line( color="steelblue") +
geom_point() +
xlab("Years") +
theme_ipsum() +
theme(axis.text.x=element_text(angle=60, hjust=1)) +
scale_x_continuous(limit=c(1984,2016))
anim = staticplot + transition_states(year_suicide$year, transition_length = 4, state_length = 1) +
view_follow(fixed_x = TRUE)  +
labs(title = 'Suicide Number : {closest_state}',
subtitle  =  "Top 10 Countries",
caption  = "Suicide Rates in 100k")
anim
anim = staticplot + transition_states(most_suicide$year, transition_length = 4, state_length = 1) +
view_follow(fixed_x = TRUE)  +
labs(title = 'Suicide Number : {closest_state}',
subtitle  =  "Top 10 Countries",
caption  = "Suicide Rates in 100k")
anim
year_suicide$year
most_suicide$year
#Country wise suicides
most_suicide <- sqldf("select country, sum(suicidesper100k) as total from suicide4 group by country ")
most_suicide$year
most_suicide %>% arrange(desc(total))
world_map <- map_data("world")
ggplot(most_suicide, aes(map_id = country)) +
geom_map(dat=world_map, map = world_map, aes(map_id=region), fill="white", color="black")+
geom_map(aes(fill = total ), map = world_map) + expand_limits(x = world_map$long, y = world_map$lat)+
scale_fill_gradient(low="#ece7f2", high="#0570b0", name="Suicides")
ggplot(most_suicide, aes(map_id = country)) +
geom_map(dat=world_map, map = world_map, aes(map_id=region), fill="white", color="black")+
geom_map(aes(fill = total ), map = world_map) + expand_limits(x = world_map$long, y = world_map$lat)+
scale_fill_gradient(low="#ece7f2", high="#0570b0", name="Suicides")
ggplot(most_suicide, aes(map_id = country)) +
geom_map(dat=world_map, map = world_map, aes(map_id=region), fill="white", color="black")+
geom_map(aes(fill = total ), map = world_map) + expand_limits(x = world_map$long, y = world_map$lat)+
scale_fill_gradient(low="#ece7f2", high="#0570b0", name="Suicides")
ggplot(most_suicide, aes(map_id = country)) +
geom_map(dat=world_map, map = world_map, aes(map_id=region), fill="white", color="black")+
geom_map(aes(fill = total ), map = world_map) + expand_limits(x = world_map$long, y = world_map$lat)+
scale_fill_gradient(low="#ece7f2", high="#0570b0", name="Suicides")
ggplot(most_suicide, aes(map_id = country)) +
geom_map(dat=world_map, map = world_map, aes(map_id=region), fill="white", color="black")+
geom_map(aes(fill = total ), map = world_map) + expand_limits(x = world_map$long, y = world_map$lat)+
scale_fill_gradient(low="#ece7f2", high="#0570b0", name="Suicides")
anim = staticplot + transition_states(year_suicide$year, transition_length = 4, state_length = 1) +
view_follow(fixed_x = TRUE)  +
labs(title = 'Suicide Number : {closest_state}',
subtitle  =  "Top 10 Countries",
caption  = "Suicide Rates in 100k")
anim
staticplot <- ggplot(year_suicide, aes(x=year, y=total)) +
geom_line( color="steelblue") +
geom_point() +
xlab("Years") +
theme_ipsum() +
theme(axis.text.x=element_text(angle=60, hjust=1)) +
scale_x_continuous(limit=c(1984,2016))
anim = staticplot + transition_states(year_suicide$year, transition_length = 4, state_length = 1) +
view_follow(fixed_x = TRUE)  +
labs(title = 'Suicide Number : {closest_state}',
subtitle  =  "Top 10 Countries",
caption  = "Suicide Rates in 100k")
anim
animate(anim, 200, fps = 20,  width = 1200, height = 1000,
renderer = gifski_renderer("gganim.gif"))
ggplot(year_suicide, aes(x=year, y=total)) +
geom_line( color="steelblue") +
geom_point() +
xlab("Years") +
theme_ipsum() +
theme(axis.text.x=element_text(angle=60, hjust=1)) +
scale_x_continuous(limit=c(1984,2016))
#Year wise suicide & gender wise
year_suicide_gender <- sqldf('select year,sex,sum(suicidesper100k) as total from suicide4 group by year,sex')
head(year_suicide_gender)
ggplot(year_suicide_gender,aes(x=year,y=total,colour=sex,group=sex)) + geom_line()
#Year wise suicide & age wise
year_suicide_age <- sqldf('select year,age,sum(suicidesper100k) as total from suicide4 group by year,age')
head(year_suicide_age)
ggplot(year_suicide_age,aes(x=year,y=total,colour=age,group=age)) + geom_line()+theme_ipsum() +
theme(axis.text.x=element_text(angle=60, hjust=1))
pie3D(age_suicide$total, labels =paste(age_suicide$age,",",age_suicide$perc,"%"), main = "An exploded 3D pie chart",
explode=0.1, radius=0.9, labelcex = 0.9,  start=0.7)
barplot(height=gender_suicide$total, names=gender_suicide$sex,col = coul)
gender_suicide
gender_suicide$perc <- gender_suicide$total/sum(gender_suicide$total)
gender_suicide
gender_suicide$perc <- round(gender_suicide$total/sum(gender_suicide$total)*100)
gender_suicide
barplot(height=gender_suicide$total, names=gender_suicide$sex,col = coul)
gender_suicide['male']
gender_suicide['gender',]
gender_suicide['male',]
gender_suicide['sex',]
gender_suicide
gender_suicide %>% filter(sex ==''male)
gender_suicide %>% filter(sex =="male")
gender_suicide$perc %>% filter(sex =="male")
gender_suicide %>% select(perc) %>% filter(sex =="male")
gender_suicide %>% filter(sex =="male")  %>% select(perc)
runApp()
#country wise gender suicides
country_gen_suicide <- sqldf('select country,sex,sum(suicidesper100k) as total from suicide4 group by country,sex')
head(country_gen_suicide)
country_gen_suicide <- country_gen_suicide  %>% arrange(desc(total))
valueBox(
paste0(country_gen_suicide[0][0]), "Male Suicides", icon = icon("mars"),
color = "blue"
)
ggplot(year_gdp_suicide,aes(x=year))+
geom_line( aes(y=GDP), size=1,color = "#69b3a2") +
geom_line( aes(y=total), size=1,color = rgb(0.2, 0.6, 0.9, 1))+
scale_y_continuous(
# Features of the first axis
name = "GDP ($) in 10^6",
# Add a second axis and specify its features
sec.axis = sec_axis(~.*coeff, name="Suicides")
) +
theme_ipsum() +
theme(
axis.title.y = element_text(color ="#69b3a2" , size=13),
axis.title.y.right = element_text(color =rgb(0.2, 0.6, 0.9, 1) , size=13)
) +
ggtitle("GDP down, suicides up")
#Calculate effect
aoveta<-sjstats::eta_sq(res2)[2]
#use the aov function - same as one way but makes it easier to access values for reporting
res2<-stats::aov(mG3~ studytime.m, data=s_perform)
setwd('../../Probablity and statistics/Portfolio/Portfolio site')
s_perform<-read_csv('sperformance-dataset.csv')
s_perform<-read_csv('../sperformance-dataset.csv')
#Get descriptive stastitics by group - output as a matrix
psych::describeBy(s_perform$mG3, s_perform$studytime.m, mat=TRUE)
#Conduct Bartlett's test for homogeneity of variance in library car - the null hypothesis is that variances in groups are equal so to assume homogeneity we would expect probability to not be statistically significant.
stats::bartlett.test(mG3~ studytime.m, data=s_perform)
#Conduct ANOVA using the userfriendlyscience test oneway
#In this case we can use Tukey as the post-hoc test option since variances in the groups are equal
#If variances were not equal we would use Games-Howell
userfriendlyscience::oneway(as.factor(s_perform$studytime.m),y=s_perform$mG3,posthoc='Tukey')
#use the aov function - same as one way but makes it easier to access values for reporting
res2<-stats::aov(mG3~ studytime.m, data=s_perform)
#Get the F statistic into a variable to make reporting easier
fstat<-summary(res2)[[1]][["F value"]][[1]]
#Get the p value into a variable to make reporting easier
aovpvalue<-summary(res2)[[1]][["Pr(>F)"]][[1]]
#Calculate effect
aoveta<-sjstats::eta_sq(res2)[2]
aoveta
#Model 2
model2<-lm(s_perform$mG3~s_perform$mG2+s_perform$higher.m)
anova(model2)
summary(model2)
stargazer(model2, type="text") #Tidy output of all the required stats
library(semTools)
library(pastecs)
library(psych)
library(FSA) #For percentage
library(car) # For Levene's test for homogeneity of variance
library(effectsize) #To calculate effect size for t-test
#install.packages('userfriendlyscience')
library(userfriendlyscience)
library('sjstats')
#install.packages('gmodels')
library(gmodels)
#install.packages('lm.beta')
#install.packages('stargazer')
library(lm.beta) #Will allow us to isolate the beta co-efficients
library(stargazer)#For formatting outputs/tables
stargazer(model2, type="text") #Tidy output of all the required stats
lm.beta(model2)
stargazer(model1, model2, type="text")
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model2))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(s_perform[influential, ])  # influential observations.
#Model
#Model1
scatter <- ggplot(s_perform, aes(s_perform$mG2, s_perform$mG3))
#Add a regression line
scatter + geom_point() + geom_smooth(method = "lm", colour = "Red", se = F) + labs(x = "mG2", y = "mG3")
#Pearson Correlation
stats::cor.test(s_perform$mG2, s_perform$mG3, method='pearson')
#Building linear model using mG1,mG2 as predictors and mG3 as the dependent variable
s_perform$mG3<-na_if(s_perform$mG3,0)
model1<-lm(s_perform$mG3~s_perform$mG2)
anova(model1)
summary(model1)
lm.beta::lm.beta(model1)
stargazer(model1, type="text") #Tidy output of all the required stats
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model1))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#finding rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
influential
stem(influential)
head(s_perform[influential, ])  # influential observations.
head(s_perform[influential, ]$mG2)  # influential observations - look at the values of mG1
head(s_perform[influential, ]$mG3)  # influential observations - look at the values of mG3
car::outlierTest(model1) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?
car::leveragePlots(model1) # leverage plots
#Assess homocedasticity
plot(model1,1)
plot(model1, 3)
#Create histogram and  density plot of the residuals
plot(density(resid(model1)))
#Create a QQ plotqqPlot(model, main="QQ Plot") #qq plot for studentized resid
car::qqPlot(model1, main="QQ Plot") #qq plot for studentized resid
#Calculate Collinearity
vifmodel<-car::vif(model1)
vifmodel
#Model 2
model2<-lm(s_perform$mG3~s_perform$mG2+s_perform$higher.m)
anova(model2)
summary(model2)
stargazer(model2, type="text") #Tidy output of all the required stats
lm.beta(model2)
stargazer(model1, model2, type="text")
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model2))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model1))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#finding rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(s_perform[influential, ])  # influential observations.
head(s_perform[influential, ]$mG2)  # influential observations - look at the values of mG2
head(s_perform[influential, ]$mG3)  # influential observations - look at the values of mG3
head(s_perform[influential, ]$higher.m)  # influential observations -look at the values of higher
car::outlierTest(model2) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?
car::leveragePlots(model2) # leverage plots
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model2))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(s_perform[influential, ])  # influential observations.
head(s_perform[influential, ]$mG2)  # influential observations - look at the values of mG2
head(s_perform[influential, ]$mG3)  # influential observations - look at the values of mG3
head(s_perform[influential, ]$higher.m)  # influential observations -look at the values of higher
car::outlierTest(model2) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?
car::leveragePlots(model2) # leverage plots
#Assess homocedasticity
plot(model2,1)
plot(model2, 3)
#A density plot of the residuals
plot(density(resid(model2)))
#Create a QQ plot qPlot(model, main="QQ Plot") #qq plot for studentized resid
car::qqPlot(model2, main="QQ Plot Model 2") #qq plot for studentized resid
#Collinearity
vifmodel<-car::vif(model2)
vifmodel
#Tolerance
1/vifmodel
#Model 3
#interaction effect
#create interaction term - adding a new variable to the dataset inthighmG2
s_perform$inthigher.m <- ifelse(s_perform$higher.m=='yes',1,0)
s_perform$inthigher.m
#s_perform$mG2<-na_if(s_perform$mG2,0)
#s_perform$mG2
s_perform$inthighmG2<-as.numeric(s_perform$inthigher.m)*s_perform$mG2
s_perform$inthighmG2
model3 <-lm(s_perform$mG3~s_perform$mG2+s_perform$higher.m+s_perform$inthighmG2)
#Tolerance
1/vifmodel
#Collinearity
vifmodel<-car::vif(model2)
vifmodel
#Tolerance
1/vifmodel
#Model 3
#interaction effect
#create interaction term - adding a new variable to the dataset inthighmG2
s_perform$inthigher.m <- ifelse(s_perform$higher.m=='yes',1,0)
s_perform$inthigher.m
#s_perform$mG2<-na_if(s_perform$mG2,0)
#s_perform$mG2
s_perform$inthighmG2<-as.numeric(s_perform$inthigher.m)*s_perform$mG2
s_perform$inthighmG2
model3 <-lm(s_perform$mG3~s_perform$mG2+s_perform$higher.m+s_perform$inthighmG2)
anova(model3)
summary(model3)
stargazer(model3, type="text") #Tidy output of all the required stats
lm.beta(model3)
stargazer(model4, model3, type="text")
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model3))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stargazer(model2, model3, type="text")
#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model3))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(s_perform[influential, ])  # influential observations.
head(s_perform[influential, ]$mG2)  # influential observations - look at the values of mG2
head(s_perform[influential, ]$mG3)  # influential observations - look at the values of mG3
head(s_perform[influential, ]$higher.m)  # influential observations -look at the values of higher
head(s_perform[influential, ]$inthighmG2)  # influential observations -look at the values of interaction var
car::outlierTest(model3) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?
car::leveragePlots(model3) # leverage plots
#Assess homocedasticity
plot(model3,1)
plot(model3, 3)
#A density plot of the residuals
plot(density(resid(model3)))
#Create a QQ plot qPlot(model, main="QQ Plot") #qq plot for studentized resid
car::qqPlot(model3, main="QQ Plot Model 3") #qq plot for studentized resid
#Collinearity
vifmodel<-car::vif(model3)
vifmodel
#Tolerance
1/vifmodel
#Model 3
#interaction effect
#create interaction term - adding a new variable to the dataset inthighmG2
s_perform$romantic.m
#Model 3
#interaction effect
#create interaction term - adding a new variable to the dataset inthighmG2
s_perform$intromantic.m<- ifelse(s_perform$romantic.m=='yes',1,0)
s_perform$intromantic.m
library(Epi)#ROC Curve
library(generalhoslem)#Needed to test assumption of linearity
library("regclass")#For confusion matrix
library(DescTools)
logmodel1 <- glm(as.factor(higher.m) ~ as.factor(sex), data = s_perform, na.action = na.exclude, family = binomial(link=logit))
#Full summary of the model
summary(logmodel1)
#Chi-square plus significance
lmtest::lrtest(logmodel1)
#Output the sensitivity, specificity, and ROC plot
Epi::ROC(form=as.factor(s_perform$higher.m) ~as.factor(s_perform$sex), plot="ROC")
#Pseudo Rsquared
DescTools::PseudoR2(logmodel1, which="CoxSnell")
DescTools::PseudoR2(logmodel1, which="Nagelkerke")
#Summary of the model with co-efficients
stargazer(logmodel1, type="text")
#Pseudo Rsquared
DescTools::PseudoR2(logmodel1, which="CoxSnell")
DescTools::PseudoR2(logmodel1, which="Nagelkerke")
#Output the sensitivity, specificity, and ROC plot
Epi::ROC(form=as.factor(s_perform$higher.m) ~as.factor(s_perform$sex), plot="ROC")
#Summary of the model with co-efficients
stargazer(logmodel1, type="text")
#Exponentiate the co-efficients
exp(coefficients(logmodel1))
## odds ratios and 95% CI
cbind(Estimate=round(coef(logmodel1),4),
OR=round(exp(coef(logmodel1)),4))
#Probability of answering yes when female
arm::invlogit(coef(logmodel1)[1]+ coef(logmodel1)[2]*0)#YES this is the same as just having the 1st co-efficient
#Probability of answering yes when male
arm::invlogit(coef(logmodel1)[1]+ coef(logmodel1)[2]*1)
generalhoslem::logitgof(as.factor(s_perform$higher.m), fitted(logmodel1))
logmodel2 <- glm(as.factor(higher.m) ~ as.factor(sex)+as.factor(romantic.m), data = s_perform, na.action = na.exclude, family = binomial(link=logit))
#Full summary of the model
summary(logmodel2)
#Chi-square plus significance
lmtest::lrtest(logmodel2)
#Pseudo Rsquared
DescTools::PseudoR2(logmodel2, which="CoxSnell")
DescTools::PseudoR2(logmodel2, which="Nagelkerke")
#Output the sensitivity, specificity, and ROC plot
Epi::ROC(form=as.factor(s_perform$higher.m) ~as.factor(s_perform$sex)+as.factor(s_perform$romantic.m), plot="ROC")
#Summary of the model with co-efficients
stargazer(logmodel2, type="text")
#Exponentiate the co-efficients
exp(coefficients(logmodel2))
## odds ratios
cbind(Estimate=round(coef(logmodel2),4),
OR=round(exp(coef(logmodel2)),4))
#romantic.m 1 has a romantic relationship, 2 not having a romantic relationship
#1. Probability of answering yes when male and  having a relationship
arm::invlogit(coef(logmodel2)[1]+ coef(logmodel2)[2]*0)
##Step 4: Decide which components to retain (PRINCIPAL COMPONENTS ANALYSIS)
#Create the scree plot
plot(pc1$values, type = "b")
#Print the variance explained by each component
pc1$Vaccounted
#Print the Eigenvalues
pc1$values
#Another way to look at eigen values plus variance explained (need to use princomp function of PCA to get right class for use with factoextra functions)
pcf=princomp(corr_df)
factoextra::get_eigenvalue(pcf)
factoextra::fviz_eig(pcf, addlabels = TRUE, ylim = c(0, 50))#Visualize the Eigenvalues
factoextra::fviz_pca_var(pcf, col.var = "black")
#Step 5: Rotation
#Apply rotation to try to refine the component structure
pc2 <-  principal(corr_df, nfactors = 4, rotate = "varimax")#Extracting 4 factors
#output the components
psych::print.psych(pc2, cut = 0.3, sort = TRUE)
#output the communalities
pc2$communality
#Step3,4
#Factor Analysis - the default here is principal axis factoring fm=pa
#If we know our data going in is normally distributed we use maximum likelihood
facsol <- psych::fa(raqMatrix, nfactors=4, obs=NA, n.iter=1, rotate="varimax", fm="pa")
#Create your scree plot
plot(facsol$values, type = "b") #scree plot
#Print the Variance accounted for by each factor/component
facsol$Vaccounted
#Output the Eigenvalues
facsol$values
#Print the components with loadings
psych::print.psych(facsol,cut=0.3, sort=TRUE)
#Print sorted list of loadings
fa.sort(facsol$loading)
#create a diagram showing the factors and how the manifest variables load
fa.diagram(facsol)
#Create your scree plot
plot(facsol$values, type = "b") #scree plot
#create a diagram showing the factors and how the manifest variables load
fa.diagram(facsol)
#Step 6
corr_df
d_data
e_data
#Output our d Alpha values
psych::alpha(d_data)
#If some items are to be reversed keyed, then either recode or get alpha to reverse code as needed by setting check.keys=TRUE (be careful with this - make sure you know it makes sense)
psych::alpha(statisticsFear, check.keys=TRUE)
#If some items are to be reversed keyed, then either recode or get alpha to reverse code as needed by setting check.keys=TRUE (be careful with this - make sure you know it makes sense)
psych::alpha(e_data, check.keys=TRUE)
psych::alpha(e_data)
psych::alpha(e_data)
R.version.string
cdat <- s_perform %>% summarise_at(vars(mG1), funs(mean(., na.rm=TRUE)))
cdat
ggplot(s_perform,aes(x=mG1))+geom_histogram(binwidth = 0.5, alpha=1, position="identity") +
geom_vline(data=cdat, aes(xintercept= mG1),
linetype="dashed", size=1)
var_of_interest <- c('Pstatus','schoolsup.m','internet','higher.m','age','mG1','mG2','mG3')
summary(s_perform[var_of_interest])
needed_packages <- c("Epi",  "generalhoslem", "regclass", "DescTools")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed, repos = "http://cran.us.r-project.org")
library(Epi)#ROC Curve
library(generalhoslem)#Needed to test assumption of linearity
library("regclass")#For confusion matrix
library(DescTools)
logmodel1 <- glm(as.factor(higher.m) ~ as.factor(sex), data = s_perform, na.action = na.exclude, family = binomial(link=logit))
needed_packages <- c("tidyverse",  "semTools", "pastecs", "psych","FSA","car","effectsize","userfriendlyscience",
"sjstats","gmodels","lm.beta","stargazer","ggplot2")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed, repos = "http://cran.us.r-project.org")
needed_packages <- c("tidyverse",  "semTools", "pastecs", "psych","FSA","car","effectsize","userfriendlyscience",
"sjstats","gmodels","lm.beta","stargazer","ggplot2")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed, repos = "http://cran.us.r-project.org")
needed_packages <- c("tidyverse",  "semTools", "pastecs", "psych","FSA","car","effectsize","userfriendlyscience",
"sjstats","gmodels","lm.beta","stargazer","ggplot2")
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]
# Install not installed packages
if(length(not_installed)) install.packages(not_installed, repos = "http://cran.us.r-project.org")
